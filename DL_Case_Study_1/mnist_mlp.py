# mnist_mlp.py
# MNIST Digit Classification using Multi-Layer Perceptron (MLP)

# -----------------------------------------
# 1. Import Required Libraries
# -----------------------------------------

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# -----------------------------------------
# 2. Load MNIST Dataset
# -----------------------------------------

(x_train, y_train), (x_test, y_test) = mnist.load_data()

print("Original Training Shape:", x_train.shape)
print("Original Test Shape:", x_test.shape)

# -----------------------------------------
# 3. Reshape and Normalize Data
# -----------------------------------------

# Convert 28x28 images into 784-length vectors
x_train = x_train.reshape(-1, 784)
x_test = x_test.reshape(-1, 784)

# Normalize pixel values (0–255 → 0–1)
x_train = x_train / 255.0
x_test = x_test / 255.0

print("Reshaped Training Shape:", x_train.shape)
print("Reshaped Test Shape:", x_test.shape)

# -----------------------------------------
# 4. Build the MLP Model
# -----------------------------------------

model = Sequential([
    Dense(128, activation='relu', input_shape=(784,)),  # First Hidden Layer
    Dense(64, activation='relu'),                       # Second Hidden Layer
    Dense(10, activation='softmax')                     # Output Layer (10 classes)
])

model.summary()

# -----------------------------------------
# 5. Compile the Model
# -----------------------------------------

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# -----------------------------------------
# 6. Train the Model
# -----------------------------------------

history = model.fit(
    x_train,
    y_train,
    epochs=10,
    batch_size=32,
    validation_split=0.1
)

# -----------------------------------------
# 7. Evaluate the Model
# -----------------------------------------

test_loss, test_accuracy = model.evaluate(x_test, y_test)

print("\nTest Loss:", test_loss)
print("Test Accuracy:", test_accuracy)

# -----------------------------------------
# 8. Visualization
# -----------------------------------------

# Plot Training vs Validation Accuracy
plt.figure()
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title("Training vs Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(["Train", "Validation"])
plt.show()

# Plot Training vs Validation Loss
plt.figure()
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title("Training vs Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(["Train", "Validation"])
plt.show()

# Plot Loss vs Epochs
plt.figure()
plt.plot(history.history['loss'])
plt.title("Loss vs Epochs")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()

# -----------------------------------------
# 9. Observations (Write in Report)
# -----------------------------------------

"""
Observation:

The model achieved approximately 97–98% accuracy on the test dataset.

Training and validation accuracy increase steadily.
Validation accuracy stabilizes after several epochs,
indicating good convergence with minimal overfitting.

ReLU activation helps the model learn non-linear digit patterns.
Softmax converts outputs into probabilities for 10 digit classes.
"""